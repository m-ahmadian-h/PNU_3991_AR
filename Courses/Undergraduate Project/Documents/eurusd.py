# -*- coding: utf-8 -*-
"""EURUSD.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WniuYW4kFrQ4Gx7h0IbyQobUzbp-V1CN
"""

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 2.x
import tensorflow as tf
from google.colab import drive
import numpy as np
import tensorflow.keras as ks
import timeit
import pandas as pd
#!pip install tpucolab
#import os
#import google as gg
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D
import matplotlib.pyplot as plt
#from multiprocessing import Pool
#import time
from tensorflow.keras.models import load_model
#import zipfile
#import pprint
#from tpucolab import *
from tensorflow.python.client import device_lib
#!cat /proc/meminfo
#!cat /proc/cpuinfo
#!pip install --upgrade tensorflow
#print('>>> tensorflow version: ***',tf.__version__,'***')
#print(device_lib.list_local_devices())

drive.mount('/content/gdrive')
!unzip "/content/gdrive/My Drive/data52.zip"
data=pd.read_csv('data52.csv',names=["v1","v2","v3","v4","v5","v6","v7","v8"])  
#datainput=np.loadtxt('data9.csv',delimiter=',',dtype='float32')

datainput  =data.get(["v1","v6","v7","v8"]).values[:]
datainput  =np.array(datainput,'float32')
input_len  =200
train_len  =15000
datax      =[]
datay      =[]
datax_train=[]
datay_train=[]
datax_test =[]
datay_test =[]
#---------------------------------------------
for i in range(input_len-1,len(datainput),1):
   datax.append( datainput[i-input_len+1:i+1,0:3] )
   datay.append( datainput[i,3]  )
datax=np.array(datax)
datay=np.array(datay)
#---------------------------------------------
for i in range(datax.shape[0]):
  for j in range(1,2):
    ma=max(datax[i,:,j])
    mi=min(datax[i,:,j])
    datax[i,:,j]=(datax[i,:,j]-mi)/(ma-mi)
#----------------------------------------------
datayy=np.zeros((datay.shape[0],2))

for i in range(datay.shape[0]):
  if   datay[i]>0.5:
    datayy[i,1]=1
  
  else:
    datayy[i,0]=1
#----------------------------------------------
'''
datax_train=datax[:train_len,:,:]
datay_train=datay[:train_len]
datax_test =datax[train_len:,:,:]
datay_test =datay[train_len:]
'''

def outo():
  neuron    = 2
  Dropout   = 0.25
  inputlayer= tf.keras.layers.Input  (shape=(input_len,3))
  layer     = tf.keras.layers.Dense  (20,activation="sigmoid"   )(inputlayer)
  
  #layer     = tf.keras.layers.Dropout(Dropout                      )(     layer)
  layer     = tf.keras.layers.Dense  (neuron  ,activation="sigmoid" )(     layer)
  #layer     = tf.keras.layers.Dense  (neuron  ,activation="sigmoid")(     layer)
  #layer     = tf.keras.layers.Dense  (neuron   )(     layer)
  #layer     = tf.keras.layers.Dropout(Dropout                      )(     layer)
  layer     = tf.keras.layers.Flatten  (  )(     layer)
  #layer     = tf.keras.layers.Dense  (neuron ,activation="sigmoid" )(     layer)
  layer     = tf.keras.layers.Dense  (1  ,activation="sigmoid"    )(     layer)
  model     = tf.keras.Model         (inputlayer ,layer)

  #model.summary()
  #ks.utils.plot_model(model, show_shapes=True, show_layer_names=False, dpi=75)
  #----------------------------------------------------------------------------------------------
  model.compile(optimizer=tf.keras.optimizers.Adam(),loss=tf.keras.losses.mse)#,metrics=['acc']
  return model
TEST=[]
model=outo()
model.summary()
history=model.fit(datax,datay,batch_size=32,epochs=500)

prte=model.predict(datax[:])
x=0
plt.plot(prte[x:x+200,0],'r')
plt.plot(datay[x:x+200],'b')

def testpredict(prte):
  t,f,su=0,0,0
  for i in range(datay.shape[0]):
    if (prte[i])>=0.5 and (datay[i])>=0.5:
      t=t+1
    elif (prte[i])<0.5 and (datay[i])<0.5:
      t=t+1
    else:
      f=f+1    
  if t+f>0:
    print(t/(t+f))
testpredict(prte)

w,u,b=mo.layers[1].get_weights()
print(w.shape,u.shape,b.shape)
np.savetxt('w.txt',w,delimiter=',')
np.savetxt('u.txt',u,delimiter=',')
np.savetxt('b.txt',b,delimiter=',')

w1,u1,b1=mo.layers[3].get_weights()
print(w1.shape,u1.shape,b1.shape)
np.savetxt('w1.txt',w1,delimiter=',')
np.savetxt('u1.txt',u1,delimiter=',')
np.savetxt('b1.txt',b1,delimiter=',')

w2,u2,b2=mo.layers[4].get_weights()
print(w2.shape,u2.shape,b2.shape)
np.savetxt('w2.txt',w2,delimiter=',')
np.savetxt('u2.txt',u2,delimiter=',')
np.savetxt('b2.txt',b2,delimiter=',')

w3,b3=mo.layers[6].get_weights()
print(w3.shape,b3.shape)
np.savetxt('w3.txt',w3,delimiter=',')
np.savetxt('b3.txt',b3,delimiter=',')

'''
model=load_model('/content/gdrive/My Drive/Save_models/multiLstm4.h5')
prte=(model.predict(datax_test[:,:,:in2_len]))
testpredict(prte)
#np.savetxt('targetbuy.txt',datay_test[:,6],delimiter=',')
#np.savetxt('buy.txt',prte,delimiter=',')
'''

data=pd.read_csv('/content/data53.csv')

x=data.get(['pips','typical','darsad'])
x=x.to_numpy()
x=x[:2700]

y=data.get(['target'])
y=y.to_numpy()
y=y[:2700]
print(x.shape,y.shape)

spi=200
xs=[]
ys=[]
for i in range(len(x)-spi):
  xs.append(x[i:i+spi])
  ys.append(y[i])
xs=np.array(xs)
ys=np.array(ys)

for i in range(len(xs)):
  xs[i,:,1]=(xs[i,:,1]-np.min(xs[i,:,1]))/(np.max(xs[i,:,1])-np.min(xs[i,:,1]))


print(xs.shape,ys.shape)

input=tf.keras.layers.Input(shape=(200,3))
d1=tf.keras.layers.Dense(20,activation='sigmoid')(input)
d2=tf.keras.layers.Dense(3,activation='sigmoid')(d1)
fl=tf.keras.layers.Flatten()(d2)
d3=tf.keras.layers.Dense(1,activation='sigmoid')(fl)
model=tf.keras.Model(input,d3)
model.summary()

model.compile(optimizer=tf.keras.optimizers.Adam(),loss=tf.keras.losses.mse)
print(xs.shape,ys.shape)
model.fit(xs,ys,epochs=500,batch_size=32,verbose=0)

def testpredict(prte,datay):
  t,f,su=0,0,0
  for i in range(len(prte)):
    if (prte[i])>0.5 and (datay[i])>0.5:
      t=t+1
    elif (prte[i])<0.5 and (datay[i])<0.5:
      t=t+1
    else:
      f=f+1    
  if t+f>0:
    print(t/(t+f))
  return  t/(t+f)
testpredict(model.predict(xs),ys)